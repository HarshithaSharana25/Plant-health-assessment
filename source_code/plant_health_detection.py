# -*- coding: utf-8 -*-
"""plant_health_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10CUSjWsuasjXGc_iUpJx4vM921tyoBHJ
"""

from google.colab import drive
drive.mount("/content/drive")

import os
import cv2
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Function to load and process a single image
def load_and_process_image(img_path):
    try:
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"Unable to load image: {img_path}")
        image_resized = cv2.resize(image, (128, 128))
        label = 0
        if "health" in img_path.lower():
            label = 0
        elif "blight" in img_path.lower():
            label = 1
        elif "common_rust" in img_path.lower():
            label = 2
        elif "gray_spot" in img_path.lower():
            label = 3
        return image_resized, label
    except Exception as e:
        print(f"Error processing image: {img_path} - {e}")
        return None

# Function to load images and extract features and labels using parallel processing
def load_data_parallel(folder_path):
    images = []
    labels = []
    img_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)
                 if filename.lower().endswith(('.jpg', '.jpeg', '.png'))]
    for img_path in img_paths:
        result = load_and_process_image(img_path)
        if result is not None:
            image, label = result
            images.append(image)
            labels.append(label)
    return np.array(images), np.array(labels)

# Load data
folder_path = "/content/drive/MyDrive/mix"
images, labels = load_data_parallel(folder_path)

# Check number of samples
n_samples = len(images)
if n_samples == 0:
    raise ValueError(f"No images found in the specified folder: {folder_path}")

# Check the distribution of labels
unique, counts = np.unique(labels, return_counts=True)
print(f"Label distribution in the dataset: {dict(zip(unique, counts))}")

if len(unique) < 2:
    raise ValueError("The dataset must contain at least two classes. Please ensure the dataset is properly labeled and contains all classes.")

# Convert labels to categorical (one-hot encoding)
labels_categorical = to_categorical(labels, num_classes=4)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels_categorical, test_size=0.2, random_state=75)

# Data augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow(X_train, y_train, batch_size=32)
validation_generator = test_datagen.flow(X_test, y_test, batch_size=32)

# Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(4, activation='softmax')  # Adjust the number of units based on the number of classes
])

# Compile the model
model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // train_generator.batch_size,
    epochs=25,
    validation_data=validation_generator,
    validation_steps=len(X_test) // validation_generator.batch_size
)

# Evaluate classifier
accuracy = model.evaluate(validation_generator)
print(f"Accuracy: {accuracy[1]}")

# Function to predict if a new image is diseased or not
def predict_image(image_path):
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"Unable to load image at path: {image_path}")
    image_resized = cv2.resize(image, (128, 128))
    image_resized = image_resized.reshape(1, 128, 128, 3) / 255.0
    prediction = model.predict(image_resized)
    return np.argmax(prediction[0])

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Function to extract features from an image
def extract_features(image):
    # Resize image to ensure consistent feature shape
    resize_shape = (128, 128)
    image_resized = cv2.resize(image, resize_shape)

    # Convert image to grayscale
    gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)

    # Apply GaussianBlur to reduce noise and detail
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    # Use Canny edge detection
    edges = cv2.Canny(blur, 50, 150)

    # Calculate histogram of oriented gradients (HOG) features
    hog = cv2.HOGDescriptor()
    h = hog.compute(gray)

    # Flatten and concatenate all features
    features = np.hstack((h.flatten(), edges.flatten()))

    return features

# Function to load images and extract features and labels
def load_data(folder_path):
    images = []
    labels = []

    # Iterate through each image in the folder
    for filename in os.listdir(folder_path):
        img_path = os.path.join(folder_path, filename)
        if img_path.lower().endswith(('.jpg', '.jpeg', '.png','.JPG')):
            try:
                # Load image
                image = cv2.imread(img_path)

                # Extract features
                features = extract_features(image)

                # Append to lists
                images.append(features)
                if "health" in filename.lower():
                    labels.append(0)  # Healthy
                elif "blight" in filename.lower():
                    labels.append(1)  # Disease 1
                elif "common_rust" in filename.lower():
                    labels.append(2)  # Disease 2
                elif "gray_spot" in filename.lower():
                    labels.append(3)  # Disease 3
            except Exception as e:
                print(f"Error loading image: {img_path} - {e}")

    return np.array(images), np.array(labels)

# Load data
folder_path = "/content/drive/MyDrive/mix"
images, labels = load_data(folder_path)

# Check number of samples
n_samples = len(images)
if n_samples == 0:
    raise ValueError(f"No images found in the specified folder: {folder_path}")

# Check the distribution of labels
unique, counts = np.unique(labels, return_counts=True)
print(f"Label distribution in the dataset: {dict(zip(unique, counts))}")

if len(unique) < 2:
    raise ValueError("The dataset must contain at least two classes. Please ensure the dataset is properly labeled and contains all classes.")

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Check distribution in the training set after the split
unique_train, counts_train = np.unique(y_train, return_counts=True)
print(f"Label distribution in the training set: {dict(zip(unique_train, counts_train))}")

# Check distribution in the test set after the split
unique_test, counts_test = np.unique(y_test, return_counts=True)
print(f"Label distribution in the test set: {dict(zip(unique_test, counts_test))}")

# Train SVM classifier
clf = SVC(kernel='linear', C=1.0, random_state=42)
clf.fit(X_train, y_train)

# Evaluate classifier
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Function to predict if a new image is diseased or not
def predict_image(image_path):
    # Load image
    image = cv2.imread(image_path)

    # Check if image is loaded properly
    if image is None:
        raise ValueError(f"Unable to load image at path: {image_path}")

    # Extract features
    features = extract_features(image)

    # Reshape features array for prediction
    features = features.reshape(1, -1)

    # Predict using the trained classifier
    prediction = clf.predict(features)

    # Return prediction (0 for healthy, 1 for disease 1, 2 for disease 2, 3 for disease 3)
    return prediction[0]

# Example usage:
new_image_path = "/content/drive/MyDrive/Corn_B.JPG"
prediction = predict_image(new_image_path)

if prediction == 0:
    print("The plant in the image is healthy.")
elif prediction == 1:
    print("The plant in the image has Blight.")
elif prediction == 2:
    print("The plant in the image has Common Rust.")
elif prediction == 3:
    print("The plant in the image has Gray Spot.")

import os
import cv2
import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Function to extract features from an image
def extract_features(image):
    # Resize image to ensure consistent feature shape
    resize_shape = (128, 128)
    image_resized = cv2.resize(image, resize_shape)

    # Convert image to grayscale
    gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)

    # Apply GaussianBlur to reduce noise and detail
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    # Use Canny edge detection
    edges = cv2.Canny(blur, 50, 150)

    # Calculate histogram of oriented gradients (HOG) features
    hog = cv2.HOGDescriptor()
    h = hog.compute(gray)

    # Flatten and concatenate all features
    features = np.hstack((h.flatten(), edges.flatten()))

    return features

# Function to load images and extract features
def load_data(folder_path):
    images = []
    image_paths = []

    # Iterate through each image in the folder
    for filename in os.listdir(folder_path):
        img_path = os.path.join(folder_path, filename)
        if img_path.lower().endswith(('.jpg', '.jpeg', '.png','.JPG')):
            try:
                # Load image
                image = cv2.imread(img_path)

                # Extract features
                features = extract_features(image)

                # Append to lists
                images.append(features)
                image_paths.append(img_path)
            except Exception as e:
                print(f"Error loading image: {img_path} - {e}")

    return np.array(images), image_paths

# Load data
folder_path = "/content/drive/MyDrive/mix"
images, image_paths = load_data(folder_path)

# Check number of samples
n_samples = len(images)
if n_samples == 0:
    raise ValueError(f"No images found in the specified folder: {folder_path}")

# Apply PCA to reduce dimensionality for visualization
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(images)

# Apply K-Means clustering
n_clusters = 4  # Assuming 4 clusters based on the original 4 classes
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(images)

# Plot the clusters
plt.figure(figsize=(10, 8))
for i in range(n_clusters):
    cluster_data = reduced_data[clusters == i]
    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=f'Cluster {i}')

plt.legend()
plt.title("K-Means Clustering of Images")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()

# Function to predict the cluster of a new image
def predict_cluster(image_path):
    # Load image
    image = cv2.imread(image_path)

    # Check if image is loaded properly
    if image is None:
        raise ValueError(f"Unable to load image at path: {image_path}")

    # Extract features
    features = extract_features(image)

    # Reshape features array for prediction
    features = features.reshape(1, -1)

    # Predict using the trained K-Means model
    cluster = kmeans.predict(features)

    # Return cluster
    return cluster[0]

# Example prediction
image_path = "/content/drive/MyDrive/Corn_B.JPG"
predicted_cluster = predict_cluster(image_path)
print(f"The image belongs to cluster: {predicted_cluster}")

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

# Function to extract features from an image
def extract_features(image):
    # Resize image to ensure consistent feature shape
    resize_shape = (128, 128)
    image_resized = cv2.resize(image, resize_shape)

    # Convert image to grayscale
    gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)

    # Apply GaussianBlur to reduce noise and detail
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    # Use Canny edge detection
    edges = cv2.Canny(blur, 50, 150)

    # Calculate histogram of oriented gradients (HOG) features
    hog = cv2.HOGDescriptor()
    h = hog.compute(gray)

    # Flatten and concatenate all features
    features = np.hstack((h.flatten(), edges.flatten()))

    return features

# Function to load images and extract features
def load_data(folder_path, labels_file):
    images = []
    labels = []
    image_paths = []

    # Read labels from file
    with open(labels_file, 'r') as f:
        labels_dict = {line.split()[0]: int(line.split()[1]) for line in f}

    # Iterate through each image in the folder
    for filename in os.listdir(folder_path):
        img_path = os.path.join(folder_path, filename)
        if img_path.lower().endswith(('.jpg', '.jpeg', '.png','.JPG')):
            try:
                # Load image
                image = cv2.imread(img_path)

                # Extract features
                features = extract_features(image)

                # Append to lists
                images.append(features)
                labels.append(labels_dict[filename])
                image_paths.append(img_path)
            except Exception as e:
                print(f"Error loading image: {img_path} - {e}")

    return np.array(images), np.array(labels), image_paths

# Load data
folder_path = "/PLEASE ADD THE PATH"
labels_file = "PLEASE ADD THE DATA FILE"  # File with image labels
images, labels, image_paths = load_data(folder_path, labels_file)

# Check number of samples
n_samples = len(images)
if n_samples == 0:
    raise ValueError(f"No images found in the specified folder: {folder_path}")

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Train a Decision Tree classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Apply PCA to reduce dimensionality for visualization
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(images)

# Plot the decision tree clusters
plt.figure(figsize=(10, 8))
for i in np.unique(labels):
    cluster_data = reduced_data[labels == i]
    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=f'Class {i}')

plt.legend()
plt.title("Decision Tree Classification of Images")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()

# Function to predict the class of a new image
def predict_class(image_path):
    # Load image
    image = cv2.imread(image_path)

    # Check if image is loaded properly
    if image is None:
        raise ValueError(f"Unable to load image at path: {image_path}")

    # Extract features
    features = extract_features(image)

    # Reshape features array for prediction
    features = features.reshape(1, -1)

    # Predict using the trained Decision Tree model
    predicted_class = clf.predict(features)

    # Return the predicted class
    return predicted_class[0]

import matplotlib.pyplot as plt
models = ['SVM', 'CNN', 'Decision Tree (CART)']
accuracies = [81, 77, 54.16]
plt.figure(figsize=(10, 6))
plt.bar(models, accuracies, color=['blue', 'green', 'red'])
plt.title('Model Accuracies')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)
plt.show()